[tool.poetry]
name = "ollama"
version = "0.0.9"
description = "Run ai models locally"
authors = ["ollama team"]
readme = "README.md"
packages = [{include = "ollama"}]
scripts = {ollama = "ollama.cmd.cli:main"}

[tool.poetry.dependencies]
python = "^3.8"
aiohttp = "^3.8.4"
aiohttp-cors = "^0.7.0"
jinja2 = "^3.1.2"
requests = "^2.31.0"
tqdm = "^4.65.0"
validators = "^0.20.0"
yaspin = "^2.3.0"
ctransformers = "^0.2.10"
fuzzywuzzy = {extras = ["speedup"], version = "^0.18.0"}

# required by llama-cpp-python
llama-cpp-python = {version = "^0.1.67", optional = true}
typing-extensions = "^4.6.3"
numpy = "^1.24.4"
diskcache = "^5.6.1"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
