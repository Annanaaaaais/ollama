.entry _Z16dequantize_blockILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q4_1EPKviiR6float2EE6__halfEvS2_PT2_i
.param .u64 _Z16dequantize_blockILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q4_1EPKviiR6float2EE6__halfEvS2_PT2_i_param_0,
.param .u64 _Z16dequantize_blockILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q4_1EPKviiR6float2EE6__halfEvS2_PT2_i_param_1,
.param .u32 _Z16dequantize_blockILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q4_1EPKviiR6float2EE6__halfEvS2_PT2_i_param_2
)
{
.reg .pred %p<2>;
.reg .b16 %rs<8>;
.reg .f32 %f<7>;
.reg .b32 %r<23>;
.reg .b64 %rd<11>;


ld.param.u64 %rd1, [_Z16dequantize_blockILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q4_1EPKviiR6float2EE6__halfEvS2_PT2_i_param_0];
ld.param.u64 %rd2, [_Z16dequantize_blockILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q4_1EPKviiR6float2EE6__halfEvS2_PT2_i_param_1];
ld.param.u32 %r2, [_Z16dequantize_blockILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q4_1EPKviiR6float2EE6__halfEvS2_PT2_i_param_2];
mov.u32 %r3, %ctaid.x;
mov.u32 %r4, %ntid.x;
mov.u32 %r5, %tid.x;
shl.b32 %r6, %r5, 1;
mad.lo.s32 %r1, %r4, %r3, %r6;
setp.ge.s32 %p1, %r1, %r2;
@%p1 bra $L__BB64_2;

cvta.to.global.u64 %rd3, %rd1;
shr.s32 %r9, %r1, 31;
shr.u32 %r10, %r9, 27;
add.s32 %r11, %r1, %r10;
and.b32 %r12, %r11, -32;
sub.s32 %r13, %r1, %r12;
shr.u32 %r14, %r13, 31;
add.s32 %r15, %r13, %r14;
shr.s32 %r16, %r15, 1;
shr.s32 %r17, %r11, 5;
mul.wide.s32 %rd4, %r17, 20;
add.s64 %rd5, %rd3, %rd4;
ld.global.nc.u32 %r7, [%rd5];

	{.reg .f16 low,high;
mov.b32 {low,high}, %r7;
mov.b16 %rs1, low;}

	
	{ cvt.f32.f16 %f1, %rs1;}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r7;
mov.b16 %rs3, high;}

	
	{ cvt.f32.f16 %f2, %rs3;}


	cvt.s64.s32 %rd6, %r16;
add.s64 %rd7, %rd5, %rd6;
ld.global.nc.u8 %rs7, [%rd7+4];
cvt.u32.u16 %r18, %rs7;
and.b32 %r19, %r18, 240;
and.b32 %r20, %r18, 15;
cvt.rn.f32.s32 %f5, %r20;
shr.u32 %r21, %r19, 4;
cvt.rn.f32.s32 %f6, %r21;
fma.rn.ftz.f32 %f3, %f1, %f5, %f2;
fma.rn.ftz.f32 %f4, %f1, %f6, %f2;
add.s32 %r22, %r12, %r16;

	{ cvt.rn.f16.f32 %rs5, %f3;}


	cvta.to.global.u64 %rd8, %rd2;
mul.wide.s32 %rd9, %r22, 2;
add.s64 %rd10, %rd8, %rd9;
st.global.u16 [%rd10], %rs5;

	{ cvt.rn.f16.f32 %rs6, %f4;}


	st.global.u16 [%rd10+32], %rs6;

$L__BB64_2:
ret;

}
