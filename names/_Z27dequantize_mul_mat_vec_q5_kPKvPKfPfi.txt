.entry _Z27dequantize_mul_mat_vec_q5_kPKvPKfPfi
.param .u64 _Z27dequantize_mul_mat_vec_q5_kPKvPKfPfi_param_0,
.param .u64 _Z27dequantize_mul_mat_vec_q5_kPKvPKfPfi_param_1,
.param .u64 _Z27dequantize_mul_mat_vec_q5_kPKvPKfPfi_param_2,
.param .u32 _Z27dequantize_mul_mat_vec_q5_kPKvPKfPfi_param_3
)
{
.reg .pred %p<25>;
.reg .b16 %rs<66>;
.reg .f32 %f<109>;
.reg .b32 %r<125>;
.reg .b64 %rd<43>;


ld.param.u64 %rd14, [_Z27dequantize_mul_mat_vec_q5_kPKvPKfPfi_param_0];
ld.param.u64 %rd15, [_Z27dequantize_mul_mat_vec_q5_kPKvPKfPfi_param_1];
ld.param.u64 %rd16, [_Z27dequantize_mul_mat_vec_q5_kPKvPKfPfi_param_2];
ld.param.u32 %r10, [_Z27dequantize_mul_mat_vec_q5_kPKvPKfPfi_param_3];
shr.s32 %r11, %r10, 31;
shr.u32 %r12, %r11, 24;
add.s32 %r13, %r10, %r12;
shr.s32 %r1, %r13, 8;
mov.u32 %r2, %tid.x;
and.b32 %r124, %r2, 1;
mov.u32 %r14, 1;
shr.u32 %r4, %r2, 4;
shl.b32 %r15, %r4, 1;
shl.b32 %r5, %r14, %r15;
cvt.u16.u32 %rs2, %r5;
shl.b16 %rs1, %rs2, 4;
setp.ge.s32 %p1, %r124, %r1;
mov.f32 %f108, 0f00000000;
@%p1 bra $L__BB8_3;

and.b32 %r16, %r2, 6;
shr.u32 %r17, %r2, 3;
and.b32 %r18, %r17, 1;
or.b32 %r19, %r18, %r16;
shl.b32 %r20, %r19, 1;
shl.b32 %r21, %r4, 5;
or.b32 %r22, %r20, %r21;
cvt.s64.s32 %rd17, %r22;
shl.b32 %r23, %r5, 1;
and.b32 %r6, %r23, 254;
and.b16 %rs3, %rs1, 255;
mul.wide.u16 %r7, %rs3, 2;
and.b32 %r24, %r2, 1;
mul.wide.u32 %rd18, %r24, 256;
add.s32 %r25, %r22, %r21;
cvt.s64.s32 %rd19, %r25;
cvta.to.global.u64 %rd20, %rd15;
add.s64 %rd21, %rd18, %rd19;
shl.b64 %rd22, %rd21, 2;
add.s64 %rd23, %rd20, %rd22;
add.s64 %rd42, %rd23, 512;
mul.wide.u32 %rd24, %r24, 176;
mul.wide.u32 %rd25, %r4, 2;
add.s64 %rd26, %rd24, %rd25;
mov.u32 %r26, %ctaid.x;
mul.lo.s32 %r27, %r1, %r26;
mul.wide.s32 %rd27, %r27, 176;
add.s64 %rd28, %rd26, %rd27;
cvta.to.global.u64 %rd29, %rd14;
add.s64 %rd30, %rd29, %rd28;
add.s64 %rd41, %rd30, 8;
add.s64 %rd31, %rd24, %rd27;
add.s64 %rd32, %rd31, %rd17;
add.s64 %rd33, %rd29, %rd32;
add.s64 %rd40, %rd33, 128;
add.s64 %rd39, %rd29, %rd31;
cvt.u64.u32 %rd34, %r20;
or.b64 %rd5, %rd34, 16;

$L__BB8_2:
ld.global.nc.u32 %r28, [%rd39];

	{.reg .f16 low,high;
mov.b32 {low,high}, %r28;
mov.b16 %rs4, low;}

	
	{ cvt.f32.f16 %f7, %rs4;}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r28;
mov.b16 %rs6, high;}

	
	{ cvt.f32.f16 %f8, %rs6;}


	ld.global.nc.u16 %rs8, [%rd41];
ld.global.nc.u16 %rs9, [%rd41+4];
and.b16 %rs10, %rs9, 3855;
ld.global.nc.u16 %rs11, [%rd41+-4];
and.b16 %rs12, %rs11, -16192;
shr.u16 %rs13, %rs12, 2;
shr.u16 %rs14, %rs9, 4;
and.b16 %rs15, %rs14, 3855;
and.b16 %rs16, %rs8, -16192;
shr.u16 %rs17, %rs16, 2;
ld.global.nc.u16 %rs18, [%rd40+-80];
ld.global.nc.u16 %rs19, [%rd40+-64];
shr.u16 %rs20, %rs18, 4;
shr.u16 %rs21, %rs19, 4;
ld.global.nc.u16 %rs22, [%rd40+-16];
ld.global.nc.u16 %rs23, [%rd40];
shr.u16 %rs24, %rs22, 4;
shr.u16 %rs25, %rs23, 4;
and.b16 %rs26, %rs8, 16128;
and.b16 %rs27, %rs8, 63;
cvt.rn.f32.u16 %f9, %rs27;
shr.u16 %rs28, %rs26, 8;
cvt.rn.f32.u16 %f10, %rs28;
or.b16 %rs29, %rs17, %rs15;
and.b16 %rs30, %rs29, 63;
cvt.rn.f32.u16 %f11, %rs30;
shr.u16 %rs31, %rs29, 8;
cvt.rn.f32.u16 %f12, %rs31;
and.b16 %rs32, %rs18, 3855;
cvt.u32.u16 %r30, %rs32;
and.b32 %r31, %r30, 15;
add.s64 %rd35, %rd39, %rd5;
ld.global.nc.u8 %rs33, [%rd35];
cvt.u32.u16 %r32, %rs33;
and.b32 %r33, %r32, 255;
and.b32 %r34, %r5, %r33;
setp.eq.s32 %p2, %r34, 0;
selp.b32 %r35, 0, 16, %p2;
or.b32 %r36, %r35, %r31;
cvt.rn.f32.s32 %f13, %r36;
ld.global.nc.f32 %f14, [%rd42+-512];
and.b16 %rs34, %rs19, 3855;
cvt.u32.u16 %r37, %rs34;
and.b32 %r38, %r37, 15;
ld.global.nc.u8 %rs35, [%rd35+16];
cvt.u32.u16 %r39, %rs35;
and.b32 %r40, %r39, 255;
and.b32 %r41, %r5, %r40;
setp.eq.s32 %p3, %r41, 0;
selp.b32 %r42, 0, 16, %p3;
or.b32 %r43, %r42, %r38;
cvt.rn.f32.s32 %f15, %r43;
ld.global.nc.f32 %f16, [%rd42+-448];
mul.ftz.f32 %f17, %f16, %f15;
fma.rn.ftz.f32 %f18, %f14, %f13, %f17;
add.ftz.f32 %f19, %f18, 0f00000000;
and.b16 %rs36, %rs20, 3855;
cvt.u32.u16 %r44, %rs36;
and.b32 %r45, %r44, 15;
and.b32 %r46, %r6, %r33;
setp.eq.s32 %p4, %r46, 0;
selp.b32 %r47, 0, 16, %p4;
or.b32 %r48, %r47, %r45;
cvt.rn.f32.s32 %f20, %r48;
ld.global.nc.f32 %f21, [%rd42+-384];
and.b16 %rs37, %rs21, 3855;
cvt.u32.u16 %r49, %rs37;
and.b32 %r50, %r49, 15;
and.b32 %r51, %r6, %r40;
setp.eq.s32 %p5, %r51, 0;
selp.b32 %r52, 0, 16, %p5;
or.b32 %r53, %r52, %r50;
cvt.rn.f32.s32 %f22, %r53;
ld.global.nc.f32 %f23, [%rd42+-320];
mul.ftz.f32 %f24, %f23, %f22;
fma.rn.ftz.f32 %f25, %f21, %f20, %f24;
add.ftz.f32 %f26, %f25, 0f00000000;
and.b16 %rs38, %rs22, 3855;
cvt.u32.u16 %r54, %rs38;
and.b32 %r55, %r54, 15;
and.b16 %rs39, %rs33, %rs1;
and.b16 %rs40, %rs39, 255;
setp.eq.s16 %p6, %rs40, 0;
selp.b32 %r56, 0, 16, %p6;
or.b32 %r57, %r56, %r55;
cvt.rn.f32.s32 %f27, %r57;
ld.global.nc.f32 %f28, [%rd42];
and.b16 %rs41, %rs23, 3855;
cvt.u32.u16 %r58, %rs41;
and.b32 %r59, %r58, 15;
and.b16 %rs42, %rs35, %rs1;
and.b16 %rs43, %rs42, 255;
setp.eq.s16 %p7, %rs43, 0;
selp.b32 %r60, 0, 16, %p7;
or.b32 %r61, %r60, %r59;
cvt.rn.f32.s32 %f29, %r61;
ld.global.nc.f32 %f30, [%rd42+64];
mul.ftz.f32 %f31, %f30, %f29;
fma.rn.ftz.f32 %f32, %f28, %f27, %f31;
add.ftz.f32 %f33, %f32, 0f00000000;
and.b16 %rs44, %rs24, 3855;
cvt.u32.u16 %r62, %rs44;
and.b32 %r63, %r62, 15;
and.b32 %r64, %r7, %r33;
setp.eq.s32 %p8, %r64, 0;
selp.b32 %r65, 0, 16, %p8;
or.b32 %r66, %r65, %r63;
cvt.rn.f32.s32 %f34, %r66;
ld.global.nc.f32 %f35, [%rd42+128];
and.b16 %rs45, %rs25, 3855;
cvt.u32.u16 %r67, %rs45;
and.b32 %r68, %r67, 15;
and.b32 %r69, %r7, %r40;
setp.eq.s32 %p9, %r69, 0;
selp.b32 %r70, 0, 16, %p9;
or.b32 %r71, %r70, %r68;
cvt.rn.f32.s32 %f36, %r71;
ld.global.nc.f32 %f37, [%rd42+192];
mul.ftz.f32 %f38, %f37, %f36;
fma.rn.ftz.f32 %f39, %f35, %f34, %f38;
add.ftz.f32 %f40, %f39, 0f00000000;
add.ftz.f32 %f41, %f14, %f16;
add.ftz.f32 %f42, %f21, %f23;
mul.ftz.f32 %f43, %f42, %f10;
fma.rn.ftz.f32 %f44, %f41, %f9, %f43;
add.ftz.f32 %f45, %f28, %f30;
fma.rn.ftz.f32 %f46, %f45, %f11, %f44;
add.ftz.f32 %f47, %f35, %f37;
fma.rn.ftz.f32 %f48, %f47, %f12, %f46;
add.ftz.f32 %f49, %f48, 0f00000000;
shr.u16 %rs46, %rs32, 8;
cvt.u32.u16 %r72, %rs46;
ld.global.nc.u8 %rs47, [%rd35+1];
cvt.u32.u16 %r73, %rs47;
and.b32 %r74, %r73, 255;
and.b32 %r75, %r5, %r74;
setp.eq.s32 %p10, %r75, 0;
selp.b32 %r76, 0, 16, %p10;
or.b32 %r77, %r76, %r72;
cvt.rn.f32.s32 %f50, %r77;
ld.global.nc.f32 %f51, [%rd42+-508];
shr.u16 %rs48, %rs34, 8;
cvt.u32.u16 %r78, %rs48;
ld.global.nc.u8 %rs49, [%rd35+17];
cvt.u32.u16 %r79, %rs49;
and.b32 %r80, %r79, 255;
and.b32 %r81, %r5, %r80;
setp.eq.s32 %p11, %r81, 0;
selp.b32 %r82, 0, 16, %p11;
or.b32 %r83, %r82, %r78;
cvt.rn.f32.s32 %f52, %r83;
ld.global.nc.f32 %f53, [%rd42+-444];
mul.ftz.f32 %f54, %f53, %f52;
fma.rn.ftz.f32 %f55, %f51, %f50, %f54;
add.ftz.f32 %f56, %f19, %f55;
shr.u16 %rs50, %rs36, 8;
cvt.u32.u16 %r84, %rs50;
and.b32 %r85, %r6, %r74;
setp.eq.s32 %p12, %r85, 0;
selp.b32 %r86, 0, 16, %p12;
or.b32 %r87, %r86, %r84;
cvt.rn.f32.s32 %f57, %r87;
ld.global.nc.f32 %f58, [%rd42+-380];
shr.u16 %rs51, %rs37, 8;
cvt.u32.u16 %r88, %rs51;
and.b32 %r89, %r6, %r80;
setp.eq.s32 %p13, %r89, 0;
selp.b32 %r90, 0, 16, %p13;
or.b32 %r91, %r90, %r88;
cvt.rn.f32.s32 %f59, %r91;
ld.global.nc.f32 %f60, [%rd42+-316];
mul.ftz.f32 %f61, %f60, %f59;
fma.rn.ftz.f32 %f62, %f58, %f57, %f61;
add.ftz.f32 %f63, %f26, %f62;
shr.u16 %rs52, %rs38, 8;
cvt.u32.u16 %r92, %rs52;
and.b16 %rs53, %rs47, %rs1;
and.b16 %rs54, %rs53, 255;
setp.eq.s16 %p14, %rs54, 0;
selp.b32 %r93, 0, 16, %p14;
or.b32 %r94, %r93, %r92;
cvt.rn.f32.s32 %f64, %r94;
ld.global.nc.f32 %f65, [%rd42+4];
shr.u16 %rs55, %rs41, 8;
cvt.u32.u16 %r95, %rs55;
and.b16 %rs56, %rs49, %rs1;
and.b16 %rs57, %rs56, 255;
setp.eq.s16 %p15, %rs57, 0;
selp.b32 %r96, 0, 16, %p15;
or.b32 %r97, %r96, %r95;
cvt.rn.f32.s32 %f66, %r97;
ld.global.nc.f32 %f67, [%rd42+68];
mul.ftz.f32 %f68, %f67, %f66;
fma.rn.ftz.f32 %f69, %f65, %f64, %f68;
add.ftz.f32 %f70, %f33, %f69;
shr.u16 %rs58, %rs44, 8;
cvt.u32.u16 %r98, %rs58;
and.b32 %r99, %r7, %r74;
setp.eq.s32 %p16, %r99, 0;
selp.b32 %r100, 0, 16, %p16;
or.b32 %r101, %r100, %r98;
cvt.rn.f32.s32 %f71, %r101;
ld.global.nc.f32 %f72, [%rd42+132];
shr.u16 %rs59, %rs45, 8;
cvt.u32.u16 %r102, %rs59;
and.b32 %r103, %r7, %r80;
setp.eq.s32 %p17, %r103, 0;
selp.b32 %r104, 0, 16, %p17;
or.b32 %r105, %r104, %r102;
cvt.rn.f32.s32 %f73, %r105;
ld.global.nc.f32 %f74, [%rd42+196];
mul.ftz.f32 %f75, %f74, %f73;
fma.rn.ftz.f32 %f76, %f72, %f71, %f75;
add.ftz.f32 %f77, %f40, %f76;
add.ftz.f32 %f78, %f51, %f53;
add.ftz.f32 %f79, %f58, %f60;
mul.ftz.f32 %f80, %f79, %f10;
fma.rn.ftz.f32 %f81, %f78, %f9, %f80;
add.ftz.f32 %f82, %f65, %f67;
fma.rn.ftz.f32 %f83, %f82, %f11, %f81;
add.ftz.f32 %f84, %f72, %f74;
fma.rn.ftz.f32 %f85, %f84, %f12, %f83;
add.ftz.f32 %f86, %f49, %f85;
and.b16 %rs60, %rs11, 16128;
and.b16 %rs61, %rs11, 63;
cvt.rn.f32.u16 %f87, %rs61;
mul.ftz.f32 %f88, %f56, %f87;
shr.u16 %rs62, %rs60, 8;
cvt.rn.f32.u16 %f89, %rs62;
fma.rn.ftz.f32 %f90, %f63, %f89, %f88;
or.b16 %rs63, %rs13, %rs10;
and.b16 %rs64, %rs63, 63;
cvt.rn.f32.u16 %f91, %rs64;
fma.rn.ftz.f32 %f92, %f70, %f91, %f90;
shr.u16 %rs65, %rs63, 8;
cvt.rn.f32.u16 %f93, %rs65;
fma.rn.ftz.f32 %f94, %f77, %f93, %f92;
mul.ftz.f32 %f95, %f7, %f94;
mul.ftz.f32 %f96, %f8, %f86;
sub.ftz.f32 %f97, %f95, %f96;
add.ftz.f32 %f108, %f108, %f97;
add.s64 %rd42, %rd42, 2048;
add.s64 %rd41, %rd41, 352;
add.s64 %rd40, %rd40, 352;
add.s64 %rd39, %rd39, 352;
add.s32 %r124, %r124, 2;
setp.lt.s32 %p18, %r124, %r1;
@%p18 bra $L__BB8_2;

$L__BB8_3:
mov.b32 %r106, %f108;
mov.u32 %r107, 31;
mov.u32 %r108, 16;
mov.u32 %r109, -1;
shfl.sync.bfly.b32 %r110|%p19, %r106, %r108, %r107, %r109;
mov.b32 %f98, %r110;
add.ftz.f32 %f99, %f108, %f98;
mov.b32 %r111, %f99;
mov.u32 %r112, 8;
shfl.sync.bfly.b32 %r113|%p20, %r111, %r112, %r107, %r109;
mov.b32 %f100, %r113;
add.ftz.f32 %f101, %f99, %f100;
mov.b32 %r114, %f101;
mov.u32 %r115, 4;
shfl.sync.bfly.b32 %r116|%p21, %r114, %r115, %r107, %r109;
mov.b32 %f102, %r116;
add.ftz.f32 %f103, %f101, %f102;
mov.b32 %r117, %f103;
mov.u32 %r118, 2;
shfl.sync.bfly.b32 %r119|%p22, %r117, %r118, %r107, %r109;
mov.b32 %f104, %r119;
add.ftz.f32 %f105, %f103, %f104;
mov.b32 %r120, %f105;
shfl.sync.bfly.b32 %r122|%p23, %r120, %r14, %r107, %r109;
mov.b32 %f106, %r122;
add.ftz.f32 %f4, %f105, %f106;
setp.ne.s32 %p24, %r2, 0;
@%p24 bra $L__BB8_5;

mov.u32 %r123, %ctaid.x;
cvta.to.global.u64 %rd36, %rd16;
mul.wide.s32 %rd37, %r123, 4;
add.s64 %rd38, %rd36, %rd37;
st.global.f32 [%rd38], %f4;

$L__BB8_5:
ret;

}
