.entry _Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii
.param .u64 _Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_0,
.param .u64 _Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_1,
.param .u64 _Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_2,
.param .u32 _Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_3,
.param .u32 _Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_4
)
{
.reg .pred %p<12>;
.reg .b16 %rs<37>;
.reg .f32 %f<53>;
.reg .b32 %r<144>;
.reg .b64 %rd<28>;


ld.param.u64 %rd5, [_Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_0];
ld.param.u64 %rd3, [_Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_1];
ld.param.u64 %rd4, [_Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_2];
ld.param.u32 %r13, [_Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_3];
ld.param.u32 %r14, [_Z22dequantize_mul_mat_vecILi32ELi2EXadL_ZN43_INTERNAL_25a57f3b_12_ggml_cuda_cu_2317e5a615dequantize_q5_1EPKviiR6float2EEEvS2_PKfPfii_param_4];
cvta.to.global.u64 %rd1, %rd5;
mov.u32 %r15, %ntid.y;
mov.u32 %r16, %ctaid.y;
mov.u32 %r17, %tid.y;
mad.lo.s32 %r1, %r16, %r15, %r17;
setp.ge.s32 %p1, %r1, %r14;
@%p1 bra $L__BB26_9;

mov.u32 %r2, %tid.x;
setp.lt.s32 %p2, %r13, 1;
mov.f32 %f50, 0f00000000;
@%p2 bra $L__BB26_7;

shl.b32 %r3, %r2, 1;
mul.lo.s32 %r4, %r1, %r13;
add.s32 %r19, %r13, -1;
shr.u32 %r20, %r19, 6;
add.s32 %r5, %r20, 1;
and.b32 %r6, %r5, 1;
setp.eq.s32 %p3, %r20, 0;
mov.f32 %f50, 0f00000000;
mov.u32 %r143, 0;
@%p3 bra $L__BB26_5;

sub.s32 %r142, %r5, %r6;
cvta.to.global.u64 %rd2, %rd3;

$L__BB26_4:
add.s32 %r26, %r143, %r3;
add.s32 %r27, %r26, %r4;
shr.s32 %r28, %r27, 31;
shr.u32 %r29, %r28, 27;
add.s32 %r30, %r27, %r29;
shr.s32 %r31, %r30, 5;
shr.s32 %r32, %r26, 31;
shr.u32 %r33, %r32, 27;
add.s32 %r34, %r26, %r33;
and.b32 %r35, %r34, -32;
sub.s32 %r36, %r26, %r35;
shr.u32 %r37, %r36, 31;
add.s32 %r38, %r36, %r37;
shr.s32 %r39, %r38, 1;
mul.wide.s32 %rd6, %r31, 24;
add.s64 %rd7, %rd1, %rd6;
ld.global.nc.u32 %r22, [%rd7];

	{.reg .f16 low,high;
mov.b32 {low,high}, %r22;
mov.b16 %rs1, low;}

	
	{ cvt.f32.f16 %f12, %rs1;}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r22;
mov.b16 %rs3, high;}

	
	{ cvt.f32.f16 %f13, %rs3;}


	ld.global.nc.u8 %rs9, [%rd7+4];
cvt.u32.u8 %r40, %rs9;
ld.global.nc.u8 %rs10, [%rd7+5];
cvt.u32.u8 %r41, %rs10;
prmt.b32 %r42, %r41, %r40, 30212;
ld.global.nc.u8 %rs11, [%rd7+6];
cvt.u32.u8 %r43, %rs11;
ld.global.nc.u8 %rs12, [%rd7+7];
cvt.u32.u8 %r44, %rs12;
prmt.b32 %r45, %r44, %r43, 30212;
prmt.b32 %r46, %r45, %r42, 4180;
shr.u32 %r47, %r46, %r39;
shl.b32 %r48, %r47, 4;
and.b32 %r49, %r48, 16;
add.s32 %r50, %r39, 12;
shr.u32 %r51, %r46, %r50;
and.b32 %r52, %r51, 16;
cvt.s64.s32 %rd8, %r39;
add.s64 %rd9, %rd7, %rd8;
ld.global.nc.u8 %rs13, [%rd9+8];
and.b16 %rs14, %rs13, 240;
and.b16 %rs15, %rs13, 15;
cvt.u32.u16 %r53, %rs15;
or.b32 %r54, %r49, %r53;
cvt.rn.f32.s32 %f16, %r54;
shr.u16 %rs16, %rs14, 4;
cvt.u32.u16 %r55, %rs16;
or.b32 %r56, %r52, %r55;
cvt.rn.f32.s32 %f17, %r56;
fma.rn.ftz.f32 %f18, %f12, %f16, %f13;
fma.rn.ftz.f32 %f19, %f12, %f17, %f13;
add.s32 %r57, %r35, %r39;
mul.wide.s32 %rd10, %r57, 4;
add.s64 %rd11, %rd2, %rd10;
ld.global.nc.f32 %f20, [%rd11];
fma.rn.ftz.f32 %f21, %f20, %f18, %f50;
ld.global.nc.f32 %f22, [%rd11+64];
fma.rn.ftz.f32 %f23, %f22, %f19, %f21;
add.s32 %r58, %r26, 64;
add.s32 %r59, %r58, %r4;
shr.s32 %r60, %r59, 31;
shr.u32 %r61, %r60, 27;
add.s32 %r62, %r59, %r61;
shr.s32 %r63, %r62, 5;
shr.s32 %r64, %r58, 31;
shr.u32 %r65, %r64, 27;
add.s32 %r66, %r58, %r65;
and.b32 %r67, %r66, -32;
sub.s32 %r68, %r58, %r67;
shr.u32 %r69, %r68, 31;
add.s32 %r70, %r68, %r69;
shr.s32 %r71, %r70, 1;
mul.wide.s32 %rd12, %r63, 24;
add.s64 %rd13, %rd1, %rd12;
ld.global.nc.u32 %r24, [%rd13];

	{.reg .f16 low,high;
mov.b32 {low,high}, %r24;
mov.b16 %rs5, low;}

	
	{ cvt.f32.f16 %f14, %rs5;}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r24;
mov.b16 %rs7, high;}

	
	{ cvt.f32.f16 %f15, %rs7;}


	ld.global.nc.u8 %rs17, [%rd13+4];
cvt.u32.u8 %r72, %rs17;
ld.global.nc.u8 %rs18, [%rd13+5];
cvt.u32.u8 %r73, %rs18;
prmt.b32 %r74, %r73, %r72, 30212;
ld.global.nc.u8 %rs19, [%rd13+6];
cvt.u32.u8 %r75, %rs19;
ld.global.nc.u8 %rs20, [%rd13+7];
cvt.u32.u8 %r76, %rs20;
prmt.b32 %r77, %r76, %r75, 30212;
prmt.b32 %r78, %r77, %r74, 4180;
shr.u32 %r79, %r78, %r71;
shl.b32 %r80, %r79, 4;
and.b32 %r81, %r80, 16;
add.s32 %r82, %r71, 12;
shr.u32 %r83, %r78, %r82;
and.b32 %r84, %r83, 16;
cvt.s64.s32 %rd14, %r71;
add.s64 %rd15, %rd13, %rd14;
ld.global.nc.u8 %rs21, [%rd15+8];
and.b16 %rs22, %rs21, 240;
and.b16 %rs23, %rs21, 15;
cvt.u32.u16 %r85, %rs23;
or.b32 %r86, %r81, %r85;
cvt.rn.f32.s32 %f24, %r86;
shr.u16 %rs24, %rs22, 4;
cvt.u32.u16 %r87, %rs24;
or.b32 %r88, %r84, %r87;
cvt.rn.f32.s32 %f25, %r88;
fma.rn.ftz.f32 %f26, %f14, %f24, %f15;
fma.rn.ftz.f32 %f27, %f14, %f25, %f15;
add.s32 %r89, %r67, %r71;
mul.wide.s32 %rd16, %r89, 4;
add.s64 %rd17, %rd2, %rd16;
ld.global.nc.f32 %f28, [%rd17];
fma.rn.ftz.f32 %f29, %f28, %f26, %f23;
ld.global.nc.f32 %f30, [%rd17+64];
fma.rn.ftz.f32 %f50, %f30, %f27, %f29;
add.s32 %r143, %r143, 128;
add.s32 %r142, %r142, -2;
setp.ne.s32 %p4, %r142, 0;
@%p4 bra $L__BB26_4;

$L__BB26_5:
setp.eq.s32 %p5, %r6, 0;
@%p5 bra $L__BB26_7;

add.s32 %r92, %r143, %r3;
add.s32 %r93, %r92, %r4;
shr.s32 %r94, %r93, 31;
shr.u32 %r95, %r94, 27;
add.s32 %r96, %r93, %r95;
shr.s32 %r97, %r96, 5;
shr.s32 %r98, %r92, 31;
shr.u32 %r99, %r98, 27;
add.s32 %r100, %r92, %r99;
and.b32 %r101, %r100, -32;
sub.s32 %r102, %r92, %r101;
shr.u32 %r103, %r102, 31;
add.s32 %r104, %r102, %r103;
shr.s32 %r105, %r104, 1;
mul.wide.s32 %rd18, %r97, 24;
add.s64 %rd19, %rd1, %rd18;
ld.global.nc.u32 %r90, [%rd19];

	{.reg .f16 low,high;
mov.b32 {low,high}, %r90;
mov.b16 %rs25, low;}

	
	{ cvt.f32.f16 %f31, %rs25;}


	
	{.reg .f16 low,high;
mov.b32 {low,high}, %r90;
mov.b16 %rs27, high;}

	
	{ cvt.f32.f16 %f32, %rs27;}


	ld.global.nc.u8 %rs29, [%rd19+4];
cvt.u32.u8 %r106, %rs29;
ld.global.nc.u8 %rs30, [%rd19+5];
cvt.u32.u8 %r107, %rs30;
prmt.b32 %r108, %r107, %r106, 30212;
ld.global.nc.u8 %rs31, [%rd19+6];
cvt.u32.u8 %r109, %rs31;
ld.global.nc.u8 %rs32, [%rd19+7];
cvt.u32.u8 %r110, %rs32;
prmt.b32 %r111, %r110, %r109, 30212;
prmt.b32 %r112, %r111, %r108, 4180;
shr.u32 %r113, %r112, %r105;
shl.b32 %r114, %r113, 4;
and.b32 %r115, %r114, 16;
add.s32 %r116, %r105, 12;
shr.u32 %r117, %r112, %r116;
and.b32 %r118, %r117, 16;
cvt.s64.s32 %rd20, %r105;
add.s64 %rd21, %rd19, %rd20;
ld.global.nc.u8 %rs33, [%rd21+8];
and.b16 %rs34, %rs33, 240;
and.b16 %rs35, %rs33, 15;
cvt.u32.u16 %r119, %rs35;
or.b32 %r120, %r115, %r119;
cvt.rn.f32.s32 %f33, %r120;
shr.u16 %rs36, %rs34, 4;
cvt.u32.u16 %r121, %rs36;
or.b32 %r122, %r118, %r121;
cvt.rn.f32.s32 %f34, %r122;
fma.rn.ftz.f32 %f35, %f31, %f33, %f32;
fma.rn.ftz.f32 %f36, %f31, %f34, %f32;
add.s32 %r123, %r101, %r105;
cvta.to.global.u64 %rd22, %rd3;
mul.wide.s32 %rd23, %r123, 4;
add.s64 %rd24, %rd22, %rd23;
ld.global.nc.f32 %f37, [%rd24];
fma.rn.ftz.f32 %f38, %f37, %f35, %f50;
ld.global.nc.f32 %f39, [%rd24+64];
fma.rn.ftz.f32 %f50, %f39, %f36, %f38;

$L__BB26_7:
mov.b32 %r124, %f50;
mov.u32 %r125, 31;
mov.u32 %r126, 16;
mov.u32 %r127, -1;
shfl.sync.bfly.b32 %r128|%p6, %r124, %r126, %r125, %r127;
mov.b32 %f40, %r128;
add.ftz.f32 %f41, %f50, %f40;
mov.b32 %r129, %f41;
mov.u32 %r130, 8;
shfl.sync.bfly.b32 %r131|%p7, %r129, %r130, %r125, %r127;
mov.b32 %f42, %r131;
add.ftz.f32 %f43, %f41, %f42;
mov.b32 %r132, %f43;
mov.u32 %r133, 4;
shfl.sync.bfly.b32 %r134|%p8, %r132, %r133, %r125, %r127;
mov.b32 %f44, %r134;
add.ftz.f32 %f45, %f43, %f44;
mov.b32 %r135, %f45;
mov.u32 %r136, 2;
shfl.sync.bfly.b32 %r137|%p9, %r135, %r136, %r125, %r127;
mov.b32 %f46, %r137;
add.ftz.f32 %f47, %f45, %f46;
mov.b32 %r138, %f47;
mov.u32 %r139, 1;
shfl.sync.bfly.b32 %r140|%p10, %r138, %r139, %r125, %r127;
mov.b32 %f48, %r140;
add.ftz.f32 %f7, %f47, %f48;
setp.ne.s32 %p11, %r2, 0;
@%p11 bra $L__BB26_9;

cvta.to.global.u64 %rd25, %rd4;
mul.wide.s32 %rd26, %r1, 4;
add.s64 %rd27, %rd25, %rd26;
st.global.f32 [%rd27], %f7;

$L__BB26_9:
ret;

}
